Metadata-Version: 2.4
Name: EXAMINE
Version: 0.1.0
Summary: EXpedient Analysis of Management Information to Notice Errors
Classifier: Programming Language :: Python :: 3
Classifier: Operating System :: OS Independent
Requires-Python: >=3.9
Description-Content-Type: text/markdown
Requires-Dist: pandas
Requires-Dist: python-dotenv

# EXAMINE

EXAMINE (**Ex**pedient **A**nalysis of **M**anagement **I**nformation to **N**otice **E**rrors) is a system that helps CCS identify potential errors in supplier-reported Management Information (MI), particularly under-reported or missing spend.

## Installation

### Python Virtual Environment

```bash
python3 -m venv venv
```

Activate:

- Linux/macOS: `source venv/bin/activate`
- Windows: `.\venv\Scripts\activate`

Install dependencies:

```bash
pip install -r requirements.txt
```

### Environment Variables

Create a local `.env` file (not committed) based on `env.example`:

```bash
cp env.example .env
```

### SQL Drivers

```bash
bash install_drivers.sh
```

## Developer Tooling (Pre-commit, Ruff, pytest)

This project uses:

- [pre-commit](https://pre-commit.com/) for running checks automatically before each commit.
- [Ruff](https://docs.astral.sh/ruff/) for fast linting.
- [pytest](https://docs.pytest.org/) for unit testing.

### Install tooling

If you already installed dependencies from `requirements.txt`, install the remaining developer tools:

```bash
python -m pip install pre-commit ruff
```

Or install all at once:

```bash
python -m pip install -r requirements.txt pre-commit ruff
```

### Set up pre-commit hooks

Install hooks locally:

```bash
pre-commit install
```

Run all hooks manually across the repository:

```bash
pre-commit run --all-files
```

### Run Ruff and pytest manually

Run Ruff:

```bash
ruff check .
```

Run tests:

```bash
pytest -q
```

CI also runs Ruff and pytest on every push.

## EXAMINE Data Analysis Pipeline

The EXAMINE data analysis pipeline is fully reproducible using **DVC**. The pipeline supports **dummy** and **live** modes via the `data_mode` parameter in `params.yaml`. Live mode requires database credentials, dummy mode runs without external access.

### Running the Pipeline

To run the pipeline, set the `data_mode` in `params.yaml` to either `dummy`:

```yaml
data_mode: dummy
```

Or `live`:

```yaml
data_mode: live
```

Then run DVC:

```bash
python -m dvc repro
```

### How it works

DVC templating passes the mode into scripts:

```
--mode ${data_mode}
--outdir data/${data_mode}
--indir data/${data_mode}
```

No Python files contain hard-coded paths.

### Pipeline Stages

| Stage | Script | Outputs |
|------|--------|---------|
| get_data | `scripts/get_data.py` | contracts.csv, mi.csv, reg_number_supplier_key.csv |
| combine | `scripts/combine_data.py` | combined.csv, unmatched.csv |
| summarise | `scripts/summarise_data.py` | summary_stats.csv, line_level.csv |

### DVC Troubleshooting

If the DVC pipeline is not running:
1. Ensure DVC is initialized: `python -m dvc init`
2. Check `params.yaml` has correct `data_mode` (dummy or live)
3. Verify all dependencies installed: `pip install -r requirements.txt`
